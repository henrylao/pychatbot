{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hanz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hanz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Modelling Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Utility Libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT:  ['notebooks', 'input', '.gitattributes', '.idea', 'README.md', 'venv', 'cache', '.git', '.ipynb_checkpoints', 'requirements.txt', '__init__.py', '.gitignore', 'docs', 'website', 'pychatbot'] \n",
      "\n",
      "INPUT:  ['dzone-intents.json', 'tim-intents.json', 'oos-eval'] \n",
      "\n",
      "CACHE:  ['chatbot_model.h5', 'oos-words.pkl', 'classes.pkl', 'words.pkl', 'oos-labels.pkl'] \n",
      "\n",
      "OOS:  ['paper.pdf', 'poster.pdf', 'README.md', 'hyperparameters.csv', 'LICENSE', '.gitignore', 'supplementary.pdf', 'clinc_logo.png', 'data'] \n",
      "\n",
      "OOS INPUT:  ['binary_undersample.json', 'binary_wiki_aug.json', 'data_full.json', 'data_small.json', 'all_wiki_sents.txt', 'data_imbalanced.json', 'data_oos_plus.json'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ROOT = utils.get_project_root()\n",
    "INPUT = ROOT / \"input\" \n",
    "CACHE = ROOT / \"cache\" # directory for saving trained models\n",
    "OOS = INPUT / \"oos-eval\"\n",
    "OOS_INPUT = OOS / \"data\"\n",
    "print(\"ROOT: \", os.listdir(ROOT),\"\\n\")\n",
    "print(\"INPUT: \", os.listdir(INPUT),\"\\n\")\n",
    "print(\"CACHE: \", os.listdir(CACHE),\"\\n\")\n",
    "print(\"OOS: \", os.listdir(OOS),\"\\n\")\n",
    "print(\"OOS INPUT: \", os.listdir(OOS_INPUT),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explatory Data Analysis: OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oos_val', 'val', 'train', 'oos_test', 'test', 'oos_train'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# load data\n",
    "with open( OOS_INPUT / \"data_full.json\", \"r\") as fh:\n",
    "    data_dict = json.load(fh)\n",
    "# df = pd.DataFrame(intents)\n",
    "train_intents = data_dict['train']\n",
    "test_intents = data_dict['test']\n",
    "data_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what expression would i use to say i love you if i were an italian',\n",
       "  'translate'],\n",
       " [\"can you tell me how to say 'i do not speak much spanish', in spanish\",\n",
       "  'translate']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_intents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what expression would i use to say i love you ...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can you tell me how to say 'i do not speak muc...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the equivalent of, 'life is good' in f...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tell me how to say, 'it is a beautiful morning...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if i were mongolian, how would i say that i am...</td>\n",
       "      <td>translate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            patterns     intent\n",
       "0  what expression would i use to say i love you ...  translate\n",
       "1  can you tell me how to say 'i do not speak muc...  translate\n",
       "2  what is the equivalent of, 'life is good' in f...  translate\n",
       "3  tell me how to say, 'it is a beautiful morning...  translate\n",
       "4  if i were mongolian, how would i say that i am...  translate"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_intents)\n",
    "df_train = pd.DataFrame(train_intents, columns = [\"patterns\", \"intent\"])\n",
    "df_train.head()\n",
    "# df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the usual patterns and intent; however, there is no response recorded. This indicates a lack of understanding as to how NLP training and test works. My initial impressions given previous experience with regression and classification tells me that there needs to be a target feature. What is the target feature(s)? The intent is a label of the pattern. Furthermore, the target feature is a translation of the predicted intent into an appropriate repsonse.<br>\n",
    "\n",
    "The more I think about the problem that NLP solves, the more I can see why it's used in the sectors that need an automated call/response system. Although code is still needed to implement the functionality within the system, the rules that define the appropriate behavior are what changes and can be costly to refactor if they are hardcoded. Using properly labelled dataset with the appropriate action labels can be easier to maintain especially when there are large growing large inputs mapped to a particular intent of use.<br> \n",
    "\n",
    "It is most definitely appropriate to load up a sample dataset to understand the structure before diving into the full dataset. The reasoning is to reduce load times as well as to reduce the computational load when applying transformations. Furthermore, the full data set may adhere to the same format; however, that is not known until the full dataset is loaded in. Thus, some preparation such as reading descriptions about the datasets is well warranted to get some insight into the data.<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(intents:list)->tuple:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    words, labels, documents = [], [], []\n",
    "    # iterate list of tuples\n",
    "    for i in intents:\n",
    "        word = nltk.word_tokenize(i[0])\n",
    "        intent = i[1]\n",
    "        words.extend(word)\n",
    "        documents.append((word, intent))\n",
    "        if intent not in labels:\n",
    "            labels.append(intent)\n",
    "    return words, labels, documents\n",
    "\n",
    "(words, labels, documents ) = tokenize(train_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Sizes\n",
      "Documents: 15000\n",
      "Labels: 150\n",
      "Words: 4831\n"
     ]
    }
   ],
   "source": [
    "ignore_chars = \"! ? , .\".split(\" \")\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) \n",
    "         for w in words if w not in ignore_chars]\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(list(set(labels)))\n",
    "# cache into binary files\n",
    "with open(CACHE / \"oos-words.pkl\", \"wb\") as fh:\n",
    "    pickle.dump(words, fh)\n",
    "with open(CACHE/\"oos-labels.pkl\", \"wb\") as fh:\n",
    "    pickle.dump(labels, fh)\n",
    "print(\"Set Sizes\")\n",
    "print(\"Documents:\",len(documents))\n",
    "print(\"Labels:\", len(labels))\n",
    "print(\"Words:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed dataset creation in 76.50162196159363s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a9850971ac5a>:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x = np.array(x)\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "Y_empty = [0]*len(labels) \n",
    "\n",
    "# considering the efficiency of this algo, is there a more efficient method? this\n",
    "# is because as the data grows, computational time grows as well\n",
    "DEBUG = False\n",
    "start = time.time()\n",
    "# iterate over each entry in documents\n",
    "for d in documents:\n",
    "    if DEBUG:\n",
    "        print(\"Word pattern\", d)\n",
    "    bag = []\n",
    "    word_patterns = d[0] # access the word pattern\n",
    "    # lemmatize the entries in the selected word pattern\n",
    "    lemma_word_patterns = [lemmatizer.lemmatize(w.lower()) for\n",
    "                    w in word_patterns]\n",
    "    if DEBUG:\n",
    "        print(\"Lemmatized:\", word_patterns)\n",
    "        if (len(lemma_word_patterns) == len(word_patterns)):\n",
    "            print(\"Identified case in which the collection of words were reduced to their core meaning\")\n",
    "    # convert the lemmatized word pattern to a numerical value\n",
    "    for _ in words:\n",
    "        bag.append(1) if _ in lemma_word_patterns else bag.append(0)\n",
    "        \n",
    "    # create the label for the labels in numeric form\n",
    "    Y_row = list(Y_empty) \n",
    "    # set the activation on the output row\n",
    "    Y_row[labels.index(d[1])] = 1\n",
    "    if DEBUG:\n",
    "        print(\"Numeric Class:\",labels.index(d[1]))\n",
    "        print(\"Y_row:\",Y_row)\n",
    "        print()\n",
    "    x.append([bag, Y_row]) # add to the training set\n",
    "end = time.time()\n",
    "print(\"Completed dataset creation in {}s\".format(end - start))\n",
    "random.shuffle(x) # not sure if this is necessary? why shuffle the data when we create the training data with all of it and there is no test set\n",
    "x = np.array(x)\n",
    "x_train, Y_train = list(x[:,0]), list(x[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed dataset loading into pandas 71.453298330307s\n",
      "Training input shape: (15000, 4831)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_x_train  = pd.DataFrame(x_train)\n",
    "end = time.time()\n",
    "print(\"Completed dataset loading into pandas {}s\".format(end - start))\n",
    "\n",
    "print(\"Training input shape:\",df_x_train.shape)\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed dataset loading into pandas 2.104707956314087s\n",
      "Training output shape (15000, 150)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_Y_train = pd.DataFrame(Y_train)\n",
    "end = time.time()\n",
    "print(\"Completed dataset loading into pandas {}s\".format(end - start))\n",
    "\n",
    "print(\"Training output shape\",df_Y_train.shape)\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideration for the library used here for wrangling data is necessary. Potential alternatives are pyspark and koalas. Pyspark is faster but does not have the same interface naming conventions as pandas. Koalas applies a pandas interface for interacting with pyspark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a deep neural network model\n",
    "model = Sequential()\n",
    "# add the 1st layer -- an input layer\n",
    "model.add(Dense(128, input_shape=(len(x_train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# add the 2nd layer -- a hidden layer?\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# add the 3rd layer -- an output layer\n",
    "model.add(Dense(len(Y_train[0]), activation='softmax'))\n",
    "# compile the model. SGD + Nesterov accel gradient yields good results \n",
    "# for the particular model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation & Freeze as Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3000/3000 [==============================] - 20s 6ms/step - loss: 4.7208 - accuracy: 0.0376\n",
      "Epoch 2/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 2.7546 - accuracy: 0.2891\n",
      "Epoch 3/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 2.2082 - accuracy: 0.4050\n",
      "Epoch 4/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 1.9971 - accuracy: 0.4575\n",
      "Epoch 5/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 1.9079 - accuracy: 0.4836\n",
      "Epoch 6/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 1.8756 - accuracy: 0.4942\n",
      "Epoch 7/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 1.8256 - accuracy: 0.5163\n",
      "Epoch 8/200\n",
      "3000/3000 [==============================] - 18s 6ms/step - loss: 1.8280 - accuracy: 0.5119\n",
      "Epoch 9/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7562 - accuracy: 0.5439\n",
      "Epoch 10/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7436 - accuracy: 0.5464\n",
      "Epoch 11/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7899 - accuracy: 0.5299\n",
      "Epoch 12/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7618 - accuracy: 0.5519\n",
      "Epoch 13/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.8041 - accuracy: 0.5466\n",
      "Epoch 14/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7568 - accuracy: 0.5541\n",
      "Epoch 15/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7569 - accuracy: 0.5580\n",
      "Epoch 16/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.8271 - accuracy: 0.5515\n",
      "Epoch 17/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7927 - accuracy: 0.5471\n",
      "Epoch 18/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7219 - accuracy: 0.5720\n",
      "Epoch 19/200\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 1.7419 - accuracy: 0.5765\n",
      "Epoch 20/200\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 1.8327 - accuracy: 0.5519\n",
      "Epoch 21/200\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 1.8798 - accuracy: 0.5478\n",
      "Epoch 22/200\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 1.8496 - accuracy: 0.5549\n",
      "Epoch 23/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.8417 - accuracy: 0.5679\n",
      "Epoch 24/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8782 - accuracy: 0.5517\n",
      "Epoch 25/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8460 - accuracy: 0.5559\n",
      "Epoch 26/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.8248 - accuracy: 0.5573\n",
      "Epoch 27/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.9073 - accuracy: 0.5547\n",
      "Epoch 28/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.9021 - accuracy: 0.5526\n",
      "Epoch 29/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.9297 - accuracy: 0.5581\n",
      "Epoch 30/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8631 - accuracy: 0.5626\n",
      "Epoch 31/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8538 - accuracy: 0.5666\n",
      "Epoch 32/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.8743 - accuracy: 0.5699\n",
      "Epoch 33/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.9497 - accuracy: 0.5530\n",
      "Epoch 34/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.8564 - accuracy: 0.5697\n",
      "Epoch 35/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.8311 - accuracy: 0.5739\n",
      "Epoch 36/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.8454 - accuracy: 0.5765\n",
      "Epoch 37/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.7837 - accuracy: 0.5875\n",
      "Epoch 38/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.8581 - accuracy: 0.5738\n",
      "Epoch 39/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.7963 - accuracy: 0.5811\n",
      "Epoch 40/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8018 - accuracy: 0.5821\n",
      "Epoch 41/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.8660 - accuracy: 0.5723\n",
      "Epoch 42/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.8706 - accuracy: 0.5704\n",
      "Epoch 43/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8173 - accuracy: 0.5855\n",
      "Epoch 44/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.8600 - accuracy: 0.5739\n",
      "Epoch 45/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8620 - accuracy: 0.5766\n",
      "Epoch 46/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7960 - accuracy: 0.5818\n",
      "Epoch 47/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.7633 - accuracy: 0.5945\n",
      "Epoch 48/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.8051 - accuracy: 0.5852\n",
      "Epoch 49/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.7781 - accuracy: 0.5944\n",
      "Epoch 50/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8184 - accuracy: 0.5897\n",
      "Epoch 51/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7508 - accuracy: 0.6023\n",
      "Epoch 52/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.8519 - accuracy: 0.5869\n",
      "Epoch 53/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.8244 - accuracy: 0.5915\n",
      "Epoch 54/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.7830 - accuracy: 0.5927\n",
      "Epoch 55/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7699 - accuracy: 0.6053\n",
      "Epoch 56/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.7841 - accuracy: 0.5933\n",
      "Epoch 57/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.7096 - accuracy: 0.6063\n",
      "Epoch 58/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6769 - accuracy: 0.6126\n",
      "Epoch 59/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.7294 - accuracy: 0.6106\n",
      "Epoch 60/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7739 - accuracy: 0.6027\n",
      "Epoch 61/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6988 - accuracy: 0.6161\n",
      "Epoch 62/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.7499 - accuracy: 0.6032\n",
      "Epoch 63/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7341 - accuracy: 0.6014\n",
      "Epoch 64/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7252 - accuracy: 0.6121\n",
      "Epoch 65/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.7784 - accuracy: 0.6036\n",
      "Epoch 66/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7810 - accuracy: 0.6033\n",
      "Epoch 67/200\n",
      "3000/3000 [==============================] - 7s 2ms/step - loss: 1.7137 - accuracy: 0.6084\n",
      "Epoch 68/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6274 - accuracy: 0.6275\n",
      "Epoch 69/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6845 - accuracy: 0.6124\n",
      "Epoch 70/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6746 - accuracy: 0.6238\n",
      "Epoch 71/200\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 1.6599 - accuracy: 0.6205\n",
      "Epoch 72/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6508 - accuracy: 0.6212\n",
      "Epoch 73/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6072 - accuracy: 0.6342\n",
      "Epoch 74/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6951 - accuracy: 0.6193\n",
      "Epoch 75/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6916 - accuracy: 0.6190\n",
      "Epoch 76/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6520 - accuracy: 0.6209\n",
      "Epoch 77/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6049 - accuracy: 0.6317\n",
      "Epoch 78/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6743 - accuracy: 0.6239\n",
      "Epoch 79/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6327 - accuracy: 0.6338\n",
      "Epoch 80/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.6132 - accuracy: 0.6395\n",
      "Epoch 81/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.5940 - accuracy: 0.6373\n",
      "Epoch 82/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6663 - accuracy: 0.6302\n",
      "Epoch 83/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6549 - accuracy: 0.6314\n",
      "Epoch 84/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.6049 - accuracy: 0.6405\n",
      "Epoch 85/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5754 - accuracy: 0.6402\n",
      "Epoch 86/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6684 - accuracy: 0.6234\n",
      "Epoch 87/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5875 - accuracy: 0.6423\n",
      "Epoch 88/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5617 - accuracy: 0.6500\n",
      "Epoch 89/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6146 - accuracy: 0.6418\n",
      "Epoch 90/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5883 - accuracy: 0.6385\n",
      "Epoch 91/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6051 - accuracy: 0.6431\n",
      "Epoch 92/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5480 - accuracy: 0.6458\n",
      "Epoch 93/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5284 - accuracy: 0.6512\n",
      "Epoch 94/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.6409 - accuracy: 0.6446\n",
      "Epoch 95/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5736 - accuracy: 0.6485\n",
      "Epoch 96/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.5579 - accuracy: 0.6491\n",
      "Epoch 97/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5070 - accuracy: 0.6542\n",
      "Epoch 98/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.5681 - accuracy: 0.6525\n",
      "Epoch 99/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.5259 - accuracy: 0.6603\n",
      "Epoch 100/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5224 - accuracy: 0.6607\n",
      "Epoch 101/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4155 - accuracy: 0.6781\n",
      "Epoch 102/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5388 - accuracy: 0.6536\n",
      "Epoch 103/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.5004 - accuracy: 0.6635\n",
      "Epoch 104/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5240 - accuracy: 0.6647\n",
      "Epoch 105/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4868 - accuracy: 0.6656\n",
      "Epoch 106/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4484 - accuracy: 0.6676\n",
      "Epoch 107/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4463 - accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.5093 - accuracy: 0.6644\n",
      "Epoch 109/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4821 - accuracy: 0.6694\n",
      "Epoch 110/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4387 - accuracy: 0.6647\n",
      "Epoch 111/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4994 - accuracy: 0.6618\n",
      "Epoch 112/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.4064 - accuracy: 0.6796\n",
      "Epoch 113/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.3356 - accuracy: 0.6928\n",
      "Epoch 114/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4153 - accuracy: 0.6752\n",
      "Epoch 115/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4846 - accuracy: 0.6649\n",
      "Epoch 116/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4431 - accuracy: 0.6779\n",
      "Epoch 117/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4505 - accuracy: 0.6729\n",
      "Epoch 118/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.5070 - accuracy: 0.6724\n",
      "Epoch 119/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.4381 - accuracy: 0.6750\n",
      "Epoch 120/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.4187 - accuracy: 0.6717\n",
      "Epoch 121/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.3701 - accuracy: 0.6791\n",
      "Epoch 122/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4567 - accuracy: 0.6706\n",
      "Epoch 123/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4037 - accuracy: 0.6872\n",
      "Epoch 124/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.3370 - accuracy: 0.6882\n",
      "Epoch 125/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.3967 - accuracy: 0.6876\n",
      "Epoch 126/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4027 - accuracy: 0.6794\n",
      "Epoch 127/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.4546 - accuracy: 0.6809\n",
      "Epoch 128/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4264 - accuracy: 0.6776\n",
      "Epoch 129/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2987 - accuracy: 0.7006\n",
      "Epoch 130/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.3464 - accuracy: 0.7020\n",
      "Epoch 131/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.3697 - accuracy: 0.6933\n",
      "Epoch 132/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.3366 - accuracy: 0.6993\n",
      "Epoch 133/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.4147 - accuracy: 0.6810\n",
      "Epoch 134/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2809 - accuracy: 0.7031\n",
      "Epoch 135/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2887 - accuracy: 0.7026\n",
      "Epoch 136/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.3204 - accuracy: 0.6924\n",
      "Epoch 137/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.3173 - accuracy: 0.6957\n",
      "Epoch 138/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2946 - accuracy: 0.7019\n",
      "Epoch 139/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2704 - accuracy: 0.7135\n",
      "Epoch 140/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2720 - accuracy: 0.7078\n",
      "Epoch 141/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2254 - accuracy: 0.7100\n",
      "Epoch 142/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.3382 - accuracy: 0.7016\n",
      "Epoch 143/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2904 - accuracy: 0.7027\n",
      "Epoch 144/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.3293 - accuracy: 0.6988\n",
      "Epoch 145/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2983 - accuracy: 0.6995\n",
      "Epoch 146/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2787 - accuracy: 0.7049\n",
      "Epoch 147/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.3294 - accuracy: 0.7021\n",
      "Epoch 148/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2321 - accuracy: 0.7147\n",
      "Epoch 149/200\n",
      "3000/3000 [==============================] - 9s 3ms/step - loss: 1.2443 - accuracy: 0.7084\n",
      "Epoch 150/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.2838 - accuracy: 0.7024\n",
      "Epoch 151/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2645 - accuracy: 0.7055\n",
      "Epoch 152/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2125 - accuracy: 0.7206\n",
      "Epoch 153/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2201 - accuracy: 0.7171\n",
      "Epoch 154/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2459 - accuracy: 0.7137\n",
      "Epoch 155/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1807 - accuracy: 0.7163\n",
      "Epoch 156/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2429 - accuracy: 0.7164\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2234 - accuracy: 0.7153\n",
      "Epoch 158/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2692 - accuracy: 0.7124\n",
      "Epoch 159/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1551 - accuracy: 0.7245\n",
      "Epoch 160/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2233 - accuracy: 0.7172\n",
      "Epoch 161/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2215 - accuracy: 0.7150\n",
      "Epoch 162/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2022 - accuracy: 0.7205\n",
      "Epoch 163/200\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 1.2065 - accuracy: 0.7169\n",
      "Epoch 164/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2071 - accuracy: 0.7218\n",
      "Epoch 165/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.2379 - accuracy: 0.7180\n",
      "Epoch 166/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2355 - accuracy: 0.7209\n",
      "Epoch 167/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1494 - accuracy: 0.7243\n",
      "Epoch 168/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1687 - accuracy: 0.7333\n",
      "Epoch 169/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2222 - accuracy: 0.7202\n",
      "Epoch 170/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1319 - accuracy: 0.7291\n",
      "Epoch 171/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1371 - accuracy: 0.7287\n",
      "Epoch 172/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1362 - accuracy: 0.7343\n",
      "Epoch 173/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1420 - accuracy: 0.7386\n",
      "Epoch 174/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1403 - accuracy: 0.7291\n",
      "Epoch 175/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1718 - accuracy: 0.7325\n",
      "Epoch 176/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1399 - accuracy: 0.7401\n",
      "Epoch 177/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0914 - accuracy: 0.7415\n",
      "Epoch 178/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0926 - accuracy: 0.7389\n",
      "Epoch 179/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1114 - accuracy: 0.7343\n",
      "Epoch 180/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1460 - accuracy: 0.7287\n",
      "Epoch 181/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1263 - accuracy: 0.7383\n",
      "Epoch 182/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0797 - accuracy: 0.7381\n",
      "Epoch 183/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1523 - accuracy: 0.7325\n",
      "Epoch 184/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1107 - accuracy: 0.7399\n",
      "Epoch 185/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0367 - accuracy: 0.7443\n",
      "Epoch 186/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.1110 - accuracy: 0.7386\n",
      "Epoch 187/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0347 - accuracy: 0.7556\n",
      "Epoch 188/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0874 - accuracy: 0.7434\n",
      "Epoch 189/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0482 - accuracy: 0.7446\n",
      "Epoch 190/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1169 - accuracy: 0.7379\n",
      "Epoch 191/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0886 - accuracy: 0.7514\n",
      "Epoch 192/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0805 - accuracy: 0.7453\n",
      "Epoch 193/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0707 - accuracy: 0.7503\n",
      "Epoch 194/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0760 - accuracy: 0.7438\n",
      "Epoch 195/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0627 - accuracy: 0.7495\n",
      "Epoch 196/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1032 - accuracy: 0.7402\n",
      "Epoch 197/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0928 - accuracy: 0.7381\n",
      "Epoch 198/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0184 - accuracy: 0.7574\n",
      "Epoch 199/200\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1045 - accuracy: 0.7448\n",
      "Epoch 200/200\n",
      "3000/3000 [==============================] - 8s 3ms/step - loss: 1.0037 - accuracy: 0.7538\n",
      "Completed model fitting in 2063.2119686603546s\n",
      "Completed model caching...\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "# train model\n",
    "start = time.time()\n",
    "hist = model.fit(np.array(x_train), \n",
    "                 np.array(Y_train),\n",
    "                 epochs=200, \n",
    "                 batch_size=5,\n",
    "                 verbose=1)\n",
    "end = time.time()\n",
    "print(\"Completed model fitting in {}s\".format(end - start))\n",
    "# cache model state into binary\n",
    "model.save(CACHE/ \"oos-chatbot_model.h5\", hist)\n",
    "print(\"Completed model caching...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
